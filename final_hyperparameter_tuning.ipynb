{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.telemetry import set_diagnostics_collection\n",
    "\n",
    "set_diagnostics_collection(send_diagnostics=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.workspace import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print('Workspace name: ' + ws.name, \n",
    "      'Azure region: ' + ws.location, \n",
    "      'Subscription id: ' + ws.subscription_id, \n",
    "      'Resource group: ' + ws.resource_group, sep = '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "from azureml.core.compute_target import ComputeTargetException\n",
    "\n",
    "# choose a name for your cluster\n",
    "cluster_name = \"compute-cluster\"\n",
    "\n",
    "try:\n",
    "    compute_target = ComputeTarget(workspace=ws, name=cluster_name)\n",
    "    print('Found existing compute target')\n",
    "except ComputeTargetException:\n",
    "    print('Creating a new compute target...')\n",
    "    compute_config = AmlCompute.provisioning_configuration(vm_size='STANDARD_D2_V2', \n",
    "                                                           max_nodes=4)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, cluster_name, compute_config)\n",
    "\n",
    "# can poll for a minimum number of nodes and for a specific timeout. \n",
    "# if no min node count is provided it uses the scale settings for the cluster\n",
    "compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "# use get_status() to get a detailed status for the current cluster. \n",
    "print(compute_target.get_status().serialize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "\n",
    "project_folder = './capstone-project'\n",
    "os.makedirs(project_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "shutil.copy('train.py', project_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create an experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "experiment_name = 'House_Price_Predication'\n",
    "experiment = Experiment(ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from azureml.core.authentication import ServicePrincipalAuthentication\n",
    "# svc_pr_password = os.environ.get(\"AZUREML_PASSWORD\")\n",
    "\n",
    "# svc_pr = ServicePrincipalAuthentication(\n",
    "#     tenant_id=\"\",\n",
    "#     service_principal_id=\"\",\n",
    "#     service_principal_password=\"\")\n",
    "\n",
    "\n",
    "# ws = Workspace(subscription_id=\"\",\n",
    "#                resource_group=\"azureML\",\n",
    "#                workspace_name=\"creditcard\",\n",
    "#                auth=svc_pr)\n",
    "\n",
    "# print(\"Found workspace {} at location {}\".format(ws.name, ws.location))\n",
    "\n",
    "\n",
    "# # ws = Workspace.from_config()\n",
    "# # ws.auth=svc_pr\n",
    "# experiment_name = 'House_Price_Predication'\n",
    "# experiment=Experiment(ws, experiment_name)\n",
    "\n",
    "# datastore=ws.get_default_datastore()\n",
    "# dataset=Dataset.Tabular.from_delimited_files(datastore.path('UI/02-09-2021_034445_UTC/BankChurners.csv'))\n",
    "\n",
    "\n",
    "# os.makedirs('data',exist_ok=True)\n",
    "# local_path='data/prepared_data.csv'\n",
    "# workspace=Workspace(ws.subscription_id,ws.resource_group,ws._workspace_name)\n",
    "# # x_df=dataset.to_pandas_dataframe().head()\n",
    "# # cat_cols=[col for col in x_df if x_df[col].dtype =='O']\n",
    "# # num_cols=[col for col in x_df if x_df[col].dtype !='O']\n",
    "# # pd.get_dummies(x_df).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an environment\n",
    "Define a conda environment YAML file with your training script dependencies and create an Azure ML environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile conda_dependencies.yml\n",
    "\n",
    "# Conda environment specification. The dependencies defined in this file will\n",
    "# be automatically provisioned for runs with userManagedDependencies=False.\n",
    "\n",
    "# Details about the Conda environment file format:\n",
    "# https://conda.io/docs/user-guide/tasks/manage-environments.html#create-env-file-manually\n",
    "\n",
    "name: project_environment\n",
    "dependencies:\n",
    "  # The python interpreter version.\n",
    "  # Currently Azure ML only supports 3.5.2 and later.\n",
    "- python=3.6.2\n",
    "\n",
    "- pip:\n",
    "  - azureml-train-automl-runtime==1.21.0\n",
    "  - inference-schema\n",
    "  - azureml-interpret==1.21.0\n",
    "  - azureml-defaults==1.21.0\n",
    "- numpy>=1.16.0,<1.19.0\n",
    "- pandas==0.25.1\n",
    "- scikit-learn==0.22.1\n",
    "- xgboost<=1.3.3\n",
    "- psutil>=5.2.2,<6.0.0\n",
    "channels:\n",
    "- anaconda\n",
    "- conda-forge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "\n",
    "env = Environment.from_conda_specification(name = 'capstone-project-env', file_path = './conda_dependencies.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(source_directory=project_folder,\n",
    "                      script='train.py',\n",
    "                      arguments=['--max_depth', '5'\n",
    "                                 ,'--learning_rate',  0.1\n",
    "                                 ,'--colsample_bytree',0.3\n",
    "                                 ,'--alpha',10\n",
    "                                 ,'--n_estimators',10],\n",
    "                      compute_target=compute_target,\n",
    "                      environment=env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run = experiment.submit(src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor The Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "RunDetails(run).show()\n",
    "#run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tune model hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.train.hyperdrive.runconfig import HyperDriveConfig\n",
    "from azureml.train.hyperdrive.sampling import RandomParameterSampling\n",
    "from azureml.train.hyperdrive.run import PrimaryMetricGoal\n",
    "from azureml.train.hyperdrive.parameter_expressions import choice,uniform, randint\n",
    "from azureml.train.hyperdrive.policy import BanditPolicy, MedianStoppingPolicy\n",
    "from azureml.train.hyperdrive.parameter_expressions import uniform, randint, choice\n",
    "from sklearn.metrics import mean_squared_error,accuracy_score\n",
    "\n",
    "early_termination_policy = BanditPolicy(slack_factor=0.01)\n",
    "another_early_termination_policy = MedianStoppingPolicy(evaluation_interval=1, delay_evaluation=5)\n",
    "\n",
    "## Hyper Parameter Optimization\n",
    "hyperparameter_grid = RandomParameterSampling({\n",
    "    '--max_depth':choice(2, 3, 5, 10),\n",
    "    '--learning_rate':choice(0.05,0.1,0.15,0.20),\n",
    "    '--colsample_bytree':choice(0.3,0.5,0.7,0.9),\n",
    "    '--alpha':choice(10,20,30,40),\n",
    "    '--n_estimators':choice(100, 500, 900, 1100)\n",
    "    }\n",
    ")\n",
    "                \n",
    "hyperdrive_config = HyperDriveConfig(run_config=src,\n",
    "                                     hyperparameter_sampling=hyperparameter_grid, \n",
    "                                     primary_metric_name='mean_squared_error',\n",
    "                                     primary_metric_goal=PrimaryMetricGoal.MAXIMIZE,\n",
    "                                     policy=early_termination_policy,\n",
    "                                     max_total_runs=10,\n",
    "                                     max_concurrent_runs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the HyperDrive run\n",
    "hyperdrive_run = experiment.submit(hyperdrive_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monitor HyperDrive runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RunDetails(hyperdrive_run).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperdrive_run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert(hyperdrive_run.get_status() == \"Completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find and register best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get your best run and save the model from that run.\n",
    "best_run = hyperdrive_run.get_best_run_by_primary_metric()\n",
    "best_run_metrics = best_run.get_metrics()\n",
    "parameter_values = best_run.get_details()['runDefinition']['arguments']\n",
    "parameter_values\n",
    "# print('Best Run Id: ', best_run.id)\n",
    "# print('\\n Accuracy:', best_run_metrics['Accuracy'])\n",
    "# print('\\n C:',parameter_values[5])\n",
    "# print('\\n max-iter:',parameter_values[3])\n",
    "#print(best_run.get_file_names())\n",
    "\n",
    "# best_run.download_file(\n",
    "#     best_run.get_file_names()[-1],\n",
    "#     output_file_path='outputs'\n",
    "# )\n",
    "best_hyperdrive_model = best_run.register_model(\n",
    "    model_name=\"best_hyperdrive_model\",\n",
    "    model_path='outputs/best_hyperdrive_model.pkl',\n",
    "   \n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieve the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'best_run' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-788b2c7e5cae>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdownload_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"outputs/best_hyperdrive_model.pkl\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"outputs/best_hyperdrive_model.pkl\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'best_run' is not defined"
     ]
    }
   ],
   "source": [
    "best_run.download_file(\"outputs/best_hyperdrive_model.pkl\", \"outputs/best_hyperdrive_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(best_hyperdrive_model._path_on_datastore, \"rb\" ) as f:\n",
    "    best_model = pickle.load(f)\n",
    "best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model.steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_test = Dataset.Tabular.from_delimited_files(path='https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv')\n",
    "df_test = dataset_test.to_pandas_dataframe()\n",
    "df_test = df_test[pd.notnull(df_test['y'])]\n",
    "\n",
    "y_test = df_test['y']\n",
    "X_test = df_test.drop(['y'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Our Best Fitted Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
